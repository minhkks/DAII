{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7zqkKKsVrpm3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MinNormSolver:\n",
        "    MAX_ITER = 250\n",
        "    STOP_CRIT = 1e-6\n",
        "\n",
        "    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n",
        "        \"\"\"\n",
        "        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n",
        "        d is the distance (objective) optimzed\n",
        "        v1v1 = <x1,x1>\n",
        "        v1v2 = <x1,x2>\n",
        "        v2v2 = <x2,x2>\n",
        "        \"\"\"\n",
        "        if v1v2 >= v1v1:\n",
        "            # Case: Fig 1, third column\n",
        "            gamma = 0.999\n",
        "            cost = v1v1\n",
        "            return gamma, cost\n",
        "        if v1v2 >= v2v2:\n",
        "            # Case: Fig 1, first column\n",
        "            gamma = 0.001\n",
        "            cost = v2v2\n",
        "            return gamma, cost\n",
        "        # Case: Fig 1, second column\n",
        "        gamma = -1.0 * ( (v1v2 - v2v2) / (v1v1+v2v2 - 2*v1v2) )\n",
        "        cost = v2v2 + gamma*(v1v2 - v2v2)\n",
        "        return gamma, cost\n",
        "\n",
        "    def _min_norm_2d(vecs, dps):\n",
        "        \"\"\"\n",
        "        Find the minimum norm solution as combination of two points\n",
        "        This solution is correct if vectors(gradients) lie in 2D\n",
        "        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n",
        "        \"\"\"\n",
        "        dmin = 1e8\n",
        "        for i in range(len(vecs)):\n",
        "            for j in range(i+1,len(vecs)):\n",
        "                if (i,j) not in dps:\n",
        "                    dps[(i, j)] = 0.0\n",
        "                    dps[(i,j)] = np.dot(vecs[i], vecs[j])\n",
        "                    dps[(j, i)] = dps[(i, j)]\n",
        "                if (i,i) not in dps:\n",
        "                    dps[(i, i)] = 0.0\n",
        "                    dps[(i,i)] = np.dot(vecs[i], vecs[i])\n",
        "                if (j,j) not in dps:\n",
        "                    dps[(j, j)] = 0.0   \n",
        "                    dps[(j, j)] = np.dot(vecs[j], vecs[j])\n",
        "                c,d = MinNormSolver._min_norm_element_from2(dps[(i,i)], dps[(i,j)], dps[(j,j)])\n",
        "                if d < dmin:\n",
        "                    dmin = d\n",
        "                    sol = [(i,j),c,d]\n",
        "        return sol, dps\n",
        "\n",
        "    def _projection2simplex(y):\n",
        "        \"\"\"\n",
        "        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n",
        "        \"\"\"\n",
        "        m = len(y)\n",
        "        sorted_y = np.flip(np.sort(y), axis=0)\n",
        "        tmpsum = 0.0\n",
        "        tmax_f = (np.sum(y) - 1.0)/m\n",
        "        for i in range(m-1):\n",
        "            tmpsum+= sorted_y[i]\n",
        "            tmax = (tmpsum - 1)/ (i+1.0)\n",
        "            if tmax > sorted_y[i+1]:\n",
        "                tmax_f = tmax\n",
        "                break\n",
        "        return np.maximum(y - tmax_f, np.zeros(y.shape))\n",
        "    \n",
        "    def _next_point(cur_val, grad, n):\n",
        "        proj_grad = grad - ( np.sum(grad) / n )\n",
        "        tm1 = -1.0*cur_val[proj_grad<0]/proj_grad[proj_grad<0]\n",
        "        tm2 = (1.0 - cur_val[proj_grad>0])/(proj_grad[proj_grad>0])\n",
        "        \n",
        "        skippers = np.sum(tm1<1e-7) + np.sum(tm2<1e-7)\n",
        "        t = 1\n",
        "        if len(tm1[tm1>1e-7]) > 0:\n",
        "            t = np.min(tm1[tm1>1e-7])\n",
        "        if len(tm2[tm2>1e-7]) > 0:\n",
        "            t = min(t, np.min(tm2[tm2>1e-7]))\n",
        "\n",
        "        next_point = proj_grad*t + cur_val\n",
        "        next_point = MinNormSolver._projection2simplex(next_point)\n",
        "        return next_point\n",
        "\n",
        "    def find_min_norm_element(vecs):\n",
        "        \"\"\"\n",
        "        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n",
        "        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n",
        "        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n",
        "        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n",
        "        \"\"\"\n",
        "        # Solution lying at the combination of two points\n",
        "        dps = {}\n",
        "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n",
        "        \n",
        "        n=len(vecs)\n",
        "        sol_vec = np.zeros(n)\n",
        "        sol_vec[init_sol[0][0]] = init_sol[1]\n",
        "        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n",
        "\n",
        "        if n < 3:\n",
        "            # This is optimal for n=2, so return the solution\n",
        "            return sol_vec , init_sol[2]\n",
        "    \n",
        "        iter_count = 0\n",
        "\n",
        "        grad_mat = np.zeros((n,n))\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                grad_mat[i,j] = dps[(i, j)]\n",
        "                \n",
        "        while iter_count < MinNormSolver.MAX_ITER:\n",
        "            grad_dir = -1.0*np.dot(grad_mat, sol_vec)\n",
        "            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n",
        "            # Re-compute the inner products for line search\n",
        "            v1v1 = 0.0\n",
        "            v1v2 = 0.0\n",
        "            v2v2 = 0.0\n",
        "            for i in range(n):\n",
        "                for j in range(n):\n",
        "                    v1v1 += sol_vec[i]*sol_vec[j]*dps[(i,j)]\n",
        "                    v1v2 += sol_vec[i]*new_point[j]*dps[(i,j)]\n",
        "                    v2v2 += new_point[i]*new_point[j]*dps[(i,j)]\n",
        "            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n",
        "            new_sol_vec = nc*sol_vec + (1-nc)*new_point\n",
        "            change = new_sol_vec - sol_vec\n",
        "            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n",
        "                return sol_vec, nd\n",
        "            sol_vec = new_sol_vec   \n",
        "        return sol_vec, nd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CFV_NJ46Vn7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from autograd import grad\n",
        "import autograd.numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "def f1(x):\n",
        "    return (x[0]+1)/(-x[0]**2+3*x[0]-x[1]**2+3*x[1]+3.5)\n",
        "def f2(x):\n",
        "    return (x[0]**2-2*x[0]+x[1]**2-8*x[1]+20)/x[1]\n",
        "\n",
        "def g1(x):\n",
        "    return -2*x[0]-x[1]+6\n",
        "def g2(x):\n",
        "    return -3*x[0]-x[1]+8\n",
        "def g3(x):\n",
        "    return -x[0]+x[1]+1\n",
        "def g4(x):\n",
        "    return x[0]-1\n",
        "def g5(x):\n",
        "    return x[1]-1\n",
        "\n",
        "g1_df = grad(g1)\n",
        "g2_df = grad(g2)\n",
        "g3_df = grad(g3)\n",
        "g4_df = grad(g4)\n",
        "g5_df = grad(g5)\n",
        "\n",
        "consp = ({'type': 'ineq',\n",
        "          'fun' : lambda x: np.array([g1(x)])},\n",
        "         {'type': 'ineq',\n",
        "          'fun' : lambda x: np.array([g2(x)])},\n",
        "          {'type': 'ineq',\n",
        "          'fun' : lambda x: np.array([g3(x)])},\n",
        "          {'type': 'ineq',\n",
        "          'fun' : lambda x: np.array([g4(x)])},\n",
        "          {'type': 'ineq',\n",
        "          'fun' : lambda x: np.array([g5(x)])},\n",
        "        )\n",
        "\n",
        "def rosen(x,y):\n",
        "    \"\"\"The Rosenbrock function\"\"\"\n",
        "    return np.sqrt(np.sum((x-y)**2))\n",
        "\n",
        "def find_min(y,n):\n",
        "    x = np.random.rand(1,n).tolist()[0]\n",
        "    res = minimize(rosen, x, args=(y), jac=\"2-point\",\n",
        "                constraints=consp,method='SLSQP', options={'disp': False})\n",
        "    return res.x"
      ],
      "metadata": {
        "id": "OpfMCQX8XPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# use autograd to calculate the gradient\n",
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "from scipy.linalg import norm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_d_paretomtl(grads,value, constraint,weights,i):\n",
        "    # calculate the gradient direction for Pareto MTL\n",
        "    nobj, dim = grads.shape\n",
        "    \n",
        "    # check active constraints\n",
        "    normalized_current_weight = weights[i]/np.linalg.norm(weights[i])\n",
        "    normalized_rest_weights = np.delete(weights, (i), axis=0) / np.linalg.norm(np.delete(weights, (i), axis=0), axis = 1,keepdims = True)\n",
        "    w = normalized_rest_weights - normalized_current_weight\n",
        "       \n",
        "    # solve QP \n",
        "    gx =  np.dot(w,value/np.linalg.norm(value))\n",
        "    idx = gx >  0\n",
        "\n",
        "    test = np.concatenate((grads, np.dot(w[idx],grads)), axis = 0)\n",
        "    if test.ndim == constraint.ndim:\n",
        "      vec =  np.concatenate((test, constraint), axis = 0)\n",
        "    else:\n",
        "      vec = test\n",
        "    \n",
        "    vec = vec/norm(vec)\n",
        "\n",
        "    # use MinNormSolver to solve QP\n",
        "    sol, nd = MinNormSolver.find_min_norm_element(vec)\n",
        "    \n",
        "    # reformulate ParetoMTL as linear scalarization method, return the weights\n",
        "    weight0 =  sol[0] + np.sum(np.array([sol[j] * w[idx][j - 2,0] for j in np.arange(2,2 + np.sum(idx))]))\n",
        "    weight1 = sol[1] + np.sum(np.array([sol[j] * w[idx][j - 2,1] for j in np.arange(2,2 + np.sum(idx))]))\n",
        "\n",
        "    num_cons = len(constraint)\n",
        "    if num_cons != 0:\n",
        "      weight_cons = sol[-num_cons:]\n",
        "      weight = np.array(list([weight0]) + list([weight1]) + weight_cons)\n",
        "    else:\n",
        "      weight = np.stack([weight0, weight1])\n",
        "    return weight\n",
        "\n",
        "\n",
        "def get_d_paretomtl_init(grads,value,constraint, weights,i):\n",
        "    # calculate the gradient direction for Pareto MTL initialization\n",
        "    nobj, dim = grads.shape\n",
        "    \n",
        "    # check active constraints\n",
        "    normalized_current_weight = weights[i]/np.linalg.norm(weights[i])\n",
        "    normalized_rest_weights = np.delete(weights, (i), axis=0) / np.linalg.norm(np.delete(weights, (i), axis=0), axis = 1,keepdims = True)\n",
        "    w = normalized_rest_weights - normalized_current_weight\n",
        "    \n",
        "    gx =  np.dot(w,value/np.linalg.norm(value))\n",
        "    idx = gx >  0\n",
        "    \n",
        "    if np.sum(idx) <= 0:\n",
        "        return np.zeros(nobj)\n",
        "    if np.sum(idx) == 1:\n",
        "        sol = np.ones(1)\n",
        "    else:\n",
        "        test = np.dot(w[idx],grads)\n",
        "        if constraint.ndim == test.ndim:\n",
        "          vec =  np.concatenate((test, constraint), axis = 0)\n",
        "        else:\n",
        "          vec = test\n",
        "        vec = vec/norm(vec)\n",
        "        \n",
        "        sol, nd = MinNormSolver.find_min_norm_element(vec)\n",
        "    \n",
        "    # calculate the weights\n",
        "    weight0 =  np.sum(np.array([sol[j] * w[idx][j ,0] for j in np.arange(0, np.sum(idx))]))\n",
        "    weight1 =  np.sum(np.array([sol[j] * w[idx][j ,1] for j in np.arange(0, np.sum(idx))]))\n",
        "\n",
        "    num_cons = len(constraint)\n",
        "    if num_cons != 0:\n",
        "      weight_cons = sol[-num_cons:]\n",
        "      weight = np.array(list([weight0]) + list([weight1]) + weight_cons)\n",
        "    else:\n",
        "      weight = np.stack([weight0, weight1])\n",
        "\n",
        "    return weight\n",
        "\n",
        "\n",
        "def circle_points(r, n):\n",
        "    # generate evenly distributed preference vector\n",
        "    circles = []\n",
        "    for r, n in zip(r, n):\n",
        "        t = np.linspace(0, 0.5 * np.pi, n)\n",
        "        x = r * np.cos(t)\n",
        "        y = r * np.sin(t)\n",
        "        circles.append(np.c_[x, y])\n",
        "    return circles\n",
        "\n",
        "def circle_points2(K, min_angle=None, max_angle=None):\n",
        "    # generate evenly distributed preference vector\n",
        "    ang0 = np.pi / 30. if min_angle is None else min_angle\n",
        "    ang1 = np.pi * 8 / 20. if max_angle is None else max_angle\n",
        "    angles = np.linspace(ang0, ang1, K, endpoint=True)\n",
        "    x = np.cos(angles)\n",
        "    y = np.sin(angles)\n",
        "    return np.c_[x, y]\n",
        "\n",
        "\n",
        "# calculate the gradients using autograd\n",
        "f1_dx = grad(f1)\n",
        "f2_dx = grad(f2)\n",
        "\n",
        "\n",
        "def pareto_mtl_search(ref_vecs,i, t_iter = 100, n_dim = 2, step_size = 2, sigma = 1, kappa = 0.95, eps = 0.01, count_check = 10):\n",
        "    \"\"\"\n",
        "    Pareto MTL\n",
        "    \"\"\"\n",
        "\n",
        "    # randomly generate one solution\n",
        "    x = np.random.uniform(0, 1, n_dim)\n",
        "    x = find_min(x, 2)\n",
        "    f_all = []\n",
        "    x_all = []\n",
        "    df_all = []\n",
        "\n",
        "    # find the initial solution\n",
        "    for t in range(int(t_iter * 0.2)):\n",
        "        f, f_dx = concave_fun_eval(x)\n",
        "        # f, f_dx = convex_fun_eval(x)\n",
        "        constraint = []\n",
        "        value_set = [[g1(x),g1_df(x)], [g2(x),g2_df(x)], [g3(x),g3_df(x)], [g4(x),g4_df(x)], [g5(x),g5_df(x)]]\n",
        "        for f_value in value_set:\n",
        "          if f_value[0]<=eps:\n",
        "            constraint.append(-f_value[1])\n",
        "        constraint = np.array(constraint)\n",
        "        weights =  get_d_paretomtl_init(f_dx,f, constraint, ref_vecs,i)\n",
        "\n",
        "        if len(weights) > len(f_dx):\n",
        "          direction_descent = -np.dot(weights[0:len(f_dx)].T,f_dx).flatten() - np.dot(weights[len(f_dx):].T, constraint).flatten()\n",
        "        else:\n",
        "          direction_descent = -np.dot(weights.T,f_dx).flatten()\n",
        "          \n",
        "        x = x + step_size * direction_descent\n",
        "        x = find_min(x, n_dim)\n",
        "        x_all.append(x)\n",
        "    \n",
        "    count = 0\n",
        "    # find the Pareto optimal solution\n",
        "    for t in range(int(t_iter * 0.8)):\n",
        "        #f, f_dx = convex_fun_eval(x)\n",
        "        f, f_dx = concave_fun_eval(x)\n",
        "        f_all.append(f)\n",
        "        df_all.append(f_dx)\n",
        "        constraint = []\n",
        "        value_set = [[g1(x),g1_df(x)], [g2(x),g2_df(x)], [g3(x),g3_df(x)], [g4(x),g4_df(x)], [g5(x),g5_df(x)]]\n",
        "        for f_value in value_set:\n",
        "          if f_value[0]<=eps:\n",
        "            constraint.append(-f_value[1])\n",
        "        constraint = np.array(constraint)\n",
        "\n",
        "        weights =  get_d_paretomtl(f_dx,f,constraint, ref_vecs,i)\n",
        "        \n",
        "        if len(weights) > len(f_dx):\n",
        "          direction_descent = -np.dot(weights[0:len(f_dx)].T,f_dx).flatten() - np.dot(weights[len(f_dx):].T, constraint).flatten()\n",
        "          x_next = x + step_size * direction_descent  #x_k+1 = x_k + alpha*s_k\n",
        "          #x_next = find_min(x_next, n_dim)\n",
        "          f_after, f_dx_after = concave_fun_eval(x_next)\n",
        "          #f_after, f_dx_after = convex_fun_eval(x_next)\n",
        "          \n",
        "          if np.dot(weights[0:len(f_dx)].T,f_after).flatten() + np.dot(weights[len(f_dx):].T, constraint).flatten() <= np.dot(weights[0:len(f_dx)].T,f).flatten() - np.dot(weights[len(f_dx):].T, constraint).flatten() + sigma*np.dot(direction_descent.T, step_size * direction_descent):\n",
        "            step_size *= kappa\n",
        "        else:\n",
        "          direction_descent = -np.dot(weights.T,f_dx).flatten()\n",
        "          x_next = x + step_size * direction_descent  #x_k+1 = x_k + alpha*s_k\n",
        "          #x_next = find_min(x_next, n_dim)\n",
        "          f_after, f_dx_after = concave_fun_eval(x_next)\n",
        "          #f_after, f_dx_after = convex_fun_eval(x_next)\n",
        "          \n",
        "          if np.dot(weights.T,f_after) <= np.dot(weights.T, f) + sigma*np.dot(direction_descent.T, step_size * direction_descent):\n",
        "            step_size *= kappa\n",
        "\n",
        "        if -1/2*norm(direction_descent)**2 >= -0.01 or -1/2*norm(direction_descent)**2 <= 0.01:\n",
        "          break\n",
        "      \n",
        "        x = x + step_size * direction_descent\n",
        "        #x = find_min(x, n_dim)\n",
        "        x_all.append(x)\n",
        "\n",
        "    return x, f, x_all, f_all, df_all\n"
      ],
      "metadata": {
        "id": "WjoFWzJzY2GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concave_fun_eval(x):\n",
        "    \"\"\"\n",
        "    return the function values and gradient values\n",
        "    \"\"\"\n",
        "    return np.stack([f1(x), f2(x)]), np.stack([f1_dx(x), f2_dx(x)])\n",
        "    \n",
        "### create the ground truth Pareto front ###\n",
        "def create_pf_concave():\n",
        "    ps2 = np.linspace(1, 4, num = 100)\n",
        "    ps1 = np.linspace(1, 9/4, num = 100)\n",
        "    pf = []\n",
        "    \n",
        "    for x1 in ps1:\n",
        "      for x2 in ps2:\n",
        "        if 2*x1+x2<=6 and 3*x1+x2<=8 and x1-x2<=1:\n",
        "          x = np.array([x1, x2])\n",
        "          f, f_dx = concave_fun_eval(x)\n",
        "          pf.append(f)\n",
        "            \n",
        "    pf = np.array(pf)\n",
        "    \n",
        "    return pf\n",
        "\n",
        "pf = create_pf_concave()\n",
        "f_value_list = []\n",
        "x_value_list = []\n",
        "\n",
        "num = 20\n",
        "\n",
        "weights = circle_points([1], [num])[0]\n",
        "#weights = circle_points2(num)\n",
        "\n",
        "for i in range(num):\n",
        "        \n",
        "    print(i)\n",
        "        \n",
        "    x, f, x_all, f_all, df_all = pareto_mtl_search(ref_vecs = weights,i = i)\n",
        "\n",
        "    #x = find_min(x, 2)   \n",
        "    # if x[0] >= 1 and x[0] <= 4 and x[1] >=1 and x[1]<=9/4 and 2*x[0]+x[1]<=6 and 3*x[0]+x[1]<=8 and x[0]-x[1]<=1:\n",
        "    f_value_list.append(f)\n",
        "    x_value_list.append(x)\n",
        "\n",
        "f_value_list = np.array(f_value_list)\n",
        "plt.scatter(pf[:,0],pf[:,1], c = 'gray')\n",
        "plt.xlabel(\"f1\")\n",
        "plt.ylabel(\"f2\")\n",
        "plt.scatter(f_value_list[:,0], f_value_list[:,1], c = 'r', s = 80)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "yPJ_1GF2ZUAH",
        "outputId": "2da106c8-5437-4765-f071-d7f41c8bbc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfoElEQVR4nO3df3Rc5Xkn8O8zMxpZIFVNkFqzIWCx60ZKPGaJ53DS2JYAs3swcuLa3e4Jp0FNmi6B9eZHU5aYQ42LIafOIe3ZdA9L683SVLsJPXsS2cYW0BJBpFgNP6TiMICUOEGpQ2MH2WxVqciaGc2zf9wZIcnz487M/THz3u/nHB1Jd+7c+1wd+3nfed/nvldUFUREFBwhvwMgIiJvMfETEQUMEz8RUcAw8RMRBQwTPxFRwET8DsCOtrY2Xbdund9hEBHVlfHx8XOq2r56e10k/nXr1mFsbMzvMIiI6oqI/EO+7RzqISIKGCZ+IqKAYeInIgoYJn4iooBh4iciCpi6qOoxUSKRwNDQEGZmZtDa2opt27YhFov5HRYRBQATv8cSiQSOHTuGVCq1tG1mZgZHjx4FACZ/InIdh3o8NDg4iIGBgRVJP2dxcRFPPvmkD1ERUdCwx++BRCKBI0eOIJPJFN1vfn7eo4iIKMiY+F1kN+ETEXmJid8l/f39mJqaKus9TU1NLkVDRPQOJn6HDQ4OVrSukIhg+/btLkRERLQSE79DEokEBgYGKnpvNBrFjh07WNFDRJ5g4ndAJcM6gNXLX7NmDebn5zE0NASA5ZxE5D4m/grlq8cvR3NzM+bm5pYqeWZmZnDs2DEATP5E5C4m/jJVW6mTS/hzc3MXvZZKpTA0NMTET0SuYuIvZnYWOHwYOHsWP0ul8M35eVxoaKjoUOFwGE1NTXkT/nIzMzMVHZ+IyC4m/nxUgYMHgQcewKIIcOECfiUSwRcyGQz39GB0yxZAxNahQqEQrrrqKkxNTZVM+gDQ2tpabfREREUx8edz8CDS99+PyMICwtlN4WQSANA9MgIAGN26teRhcsM65Uz8btu2rexwiYjKEYzEv2zIBmvXArt2AS0tF+02ODiIl0dHcddDD6Ehnc57qGgqhe6REbxw3XVINTbm3UdEoKq2evjLdXR0cHyfiFxnduJfNmSDcBi4cAFYswa44w5g3z4kentx5OjRFRO1GycmkAkVX7tORdA1OYmXr7lmxXYRgYhUNPHb0dGBvr6+st9HRFQu1xK/iDwKYAeAN1V1Q3bbQwA+AiAJ4CcAPqmq/+RWDDh4EHjwQWD54mfZXnhy/378YmgImVVDNs1zc4gU6O3nRNJpNM/OLv2e6+HnvsrV0NCAqakp3H///WhqasL27dvZ8yci17i5LPPXAdy8atvTADao6kYAPwJwj2tnn521evpvv5335dyQTcPCwortc83NSEeKt4fpSARzLS0IhUJoa2urKNkvt/xegPn5eRw9ehSJRKKqYxIRFeJa4lfVEQBvrdr2t6qa604/B+AKt86Pw4et4Z0ickM2y012dSFUYqhGVDG9ZQsymQzOnTtXdairLS4uLt3JS0TkND8fxPK7AAo+eUREbheRMREZm56eLv/oZ89aY/pFrB6yAYBkYyOGe3qQLFCvn4pGMdLdjTOr3uc01vMTkVt8Sfwici+ANIBvFNpHVQ+palxV4+3t7eWfZO1aayK3iNyQzWqjW7ZgpLsbqUgEC9EoFkMhLESjSEUiGN661arjdxnr+YnILZ5X9YjIJ2BN+m7TagfHi9m1y6reKRaLKiY6O/O8IPh5Xx+GPv1pzD/2GJpnZzHX0oKJzs6CJZxOCofDrOcnItd4mvhF5GYAdwPoUdX8s65OaWkB9u2zqnryTPAmGxqsXn02kYfDYezcuRMAcOzYMUxNTWEKAFaVbLotFwereojILW6Wcz4G4HoAbSLyBoD9sKp4GgE8LdaSB8+pavFueTX27rW+r67jX1xEdN8+3LR3L24SWVpps9L19J20/KHrTP5E5AZxc7TFKfF4XCt5qtWS2VngyBHgzBng8svxzfl5nDpzxrkAXcCePxFVS0TGVTW+ervZd+4izzLKr7/ub0A25Uo6mfiJyGlGJ/5qHodYC1jSSURu8LOO33X1fhMUSzqJyA1GJ/567jGzpJOI3GJ04q/XHjMndonITUYn/kp7zGLz6VpuyZV0cqE2InKD0Yk/Foth9+7dCBVZX7+jowPx+Mpqp1ooceUqnUTklmDU8a/S399f1uMQ/dTa2orPf/7zfodBRHUosHX8g4ODcLLR8Fo9T1ATUW0yeqin3pM+UL8T1ERUu4xO/OPj436HUBWWdBKRG4xO/PUwf1EMq3uIyA1GJ36/yzKdwOoeInKa0Yl/06ZNfofgCD6Dl4icZHTi7+3tvahGv16xuoeInGJ04geA8+fP+x2CI1jdQ0ROMTrx19ONWsWwuoeInGR04jch6QNW4icicorRid8UyWSSlT1E5Bgm/jrByh4icorRib+jo8PvEBzFyh4icoLRib+vr8+o5M/KHiJygtGJHwCuvfbaouvx15P5+XmO8xNR1VzLiCLyqIi8KSKvLNv2bhF5WkROZb+/y63zA0AikcDAwAAymYybp/EMJ3mJyAludoW/DuDmVdv2AhhS1fUAhrK/u+b48eNuHt4XnOQlomq5lvhVdQTAW6s27wTwV9mf/wrAb7h1fsDqIZuIk7xEVA2vB79/VVXPZH8+C+BXPT6/ETjJS0TV8G3WU63F8gsumC8it4vImIiMTU9PexhZ7Vu/fr3fIRBRHfM68f9CRC4HgOz3NwvtqKqHVDWuqvH29vaKTmbKypyr/eAHP+AELxFVzOvE/ziA38n+/DsAjrp5siuvvNLNw/smlUpxgpeIKuZmOedjAL4P4H0i8oaIfArAQQD/TkROAbgp+7srcqWcpuIELxFVKuLWgVX11gIvebK+sOk9Yk7wElGlzLilNQ/Te8Rcn5+IKmVs4je5R9zW1oZYLOZ3GERUp4xN/Cb3iGdmZljVQ0QVMzbxx2Ix7N6925gF2pZjVQ8RVcO8rLjM6dOnjVmgbTXT5zCIyD3GJv7BwUGMjY35HYZrTJ7DICJ3GZv4x8fH/Q7BVSbPYRCRu4xN/NZSQGaKx+Os6iGiihmb+EXE7xBcEQ6HjV2Kgoi8YWzi37Rpk98huIIPYiGiahmb+Ht7e9HW1uZ3GK5gRQ8RVcPYxD84OIhz5875HYYrWNFDRNUwNvGbXNXDih4iqoaxid/Uqh5W9BBRtYxN/CZW9YRCIVb0EFHVjE38Jlb1ZDIZVvQQUdWMTfznz5/3OwRXsKKHiKplbOKfmpryOwRXsKKHiKplbOI3kYiwooeIqsbEX0dMnLAmIu8Zm/g7Ojr8DsFxnNwlIicYm/j7+vrQ2NjodxiO4+QuEVXL2MTf39+PhYUFv8NwHCd3iahaxiZ+E6t6wuEwJ3eJqGq+JH4R+X0ReVVEXhGRx0RkjR9x1Jtrr72WyzUQUdU8T/wi8h4AnwUQV9UNAMIAPuZ1HPXo1KlTfodARAbwa6gnAqBJRCIALgHwc6dPYGJVDyd2icgJnid+Vf1HAF8BcBrAGQAzqvq3q/cTkdtFZExExqanp70OsyZxYpeInODHUM+7AOwE0AHgXwG4VEQ+vno/VT2kqnFVjbe3t5d9HtMmdzmxS0RO8WOo5yYAU6o6raopAAMAPuxDHHWFE7tE5BQ/Ev9pAB8SkUvEWoNgG4AJH+KoK5zYJSKn+DHG/zyAbwH4ewCJbAyHnD6PaQ9a58QuETkl4sdJVXU/gP1uHT+RSBj3oHVO7BKRU4y8c9fEhcw4sUtETjEy8Zs2LNLR0cGJXSJyjJGJ37RhkbfeesvvEIjIIEYm/vXr1/sdgqNM+wRDRP4yMvGbVvpo2icYIvKXkYnftB4yJ3aJyElGJn7Tesic2CUiJxmZ+NlDJiIqzMjEbxJrVQsiIucYmfhNuoHrsssu8zsEIjKMkYnfpMnd8+fP+x0CERmmYOIXkZiIPCciPxORQ9l19HOvveBNeJUxaXJXVf0OgYgMU6zH/wiAPwIQA/AjACdE5F9nX2twOa6qmHQDF8f4ichpxVbn/CVVfSr781dEZBzAUyJyG4Ca7oaadAPXunXr/A6BiAxTLPFnRKRVVWcAQFWfFZHfBPBtAO/2JLoKmTTGz3V6iMhpxYZ6YgC6RORzuQ2q+jKsJ2YNuB1YNUwa4zepESOi2lAs8f8Q1mMSf1dE3iUi7xaRdwOYA/BFT6KrkElj/CY1YkRUG4oN9TwCYAjA1QDGASyfZdTs9ppkyhh/KBTiXchE5LiCPX5V/e+q2gXgUVW9WlU7ln3VbNIHzBkeaWxs5Do9ROS4kjdwqeqdXgTiJFOGR+bn5/0OgYgMZOSdu6aM8ZvSgBFRbTEy8Zsyxs/xfSJyg5GJ35Qxfo7vE5EbfEn8IvLLIvItEZkUkQkR+XUnj9/U1OTk4YiIjFKsnNNNXwXwlKr+BxGJArjEpzhqFtfoISK3eJ74RaQVQDeATwCAqiYBJJ08hwnVMJGIX20yEZnOj6GeDgDTAP5SRF4Ska+JyKWrdxKR20VkTETGpqenyzqBCdUwqVTK7xCIyFB+JP4IgA8CeERVrwXwLwD2rt5JVQ+palxV4+3t7WWdwIRyTs5TEJFb/Ej8bwB4Q1Wfz/7+LVgNgWNMKeckInKD5wPJqno2+1Sv96nqD2Gt9vmak+fIV84ZXVhA58QEmufmMNfcjMmuLiQbG508raNMmKcgotrk1wziZwB8I1vR8zqATzp58KampncSpyo2nziBnuFhZEIhRNJppCMR7Dh+HMM9PRjdsgWowQoaE+YpiKg2+ZL4VfUkgLgX59p84gS6R0bQkE4vbQsnrSKi7pERAMDo1q1ehFIW3rVLRG4x8s7dXG8/urCAnuFhRAtUyERTKatRWFjwMjxbeNcuEbnFyMSfq4jpnJhAJlT8ElUEXZOTXoRFRFQTjEz8Oc1zc4gsG+LJJ5JOo3l21qOI7OFdu0TkJiMTf26oZ665GekSd8CmIxHMtbR4EZZtvGuXiNxkZOLPDfVMdnUhlMkU3VdUMdHZ6UVYtvGuXSJyk5GJPyfZ2Ijhnh4kGxryv97QgJHubqRqrJ6fd+0SkZuMHFNYfvPT6JYtAHBRHX8ok8FId/fS60REQWFk4l9xA5cIRrduxYvXXYfOyUk0z85irqUFE52dNdfTz+Fdu0TkJiMTfz7Jxka8fM01fodhC+/aJSI3GTnGX+89Zt61S0RuMjLx13uPmXftEpGbjEz87DETERVmZOJnj5mIqDAjE38943INROQ2Jv4aw+UaiMhtTPw1hss1EJHbmPhrDId6iMhtTPw1RlX9DoGIDMfEX2Pq/R4EIqp9TPw1hvcgEJHbmPhrDO9BICK3MfETEQUMEz8RUcD4lvhFJCwiL4nIcb9iICIKIj97/J8DMOHj+YmIAsmXxC8iVwDoBfA1t87Bskgiovz86vH/NwB3A8gU2kFEbheRMREZm56eLvsELIskIsrP88QvIjsAvKmq48X2U9VDqhpX1Xh7e3vZ52FZJBFRfn70+DcD+KiI/BTAXwO4UUT+jw9xEBEFkueJX1XvUdUrVHUdgI8BeEZVP+51HEREQcU6/hrCCWki8oKvT/1Q1e8C+K6fMdQSTkgTkRfY468hnJAmIi8w8RMRBYzRib+exsz55C0i8orRib+exsz55C0i8orRib+exszZ4ycirxid+OsJe/xE5BUmfiKigDE+8dfLBG9TU5PfIRBRQBif+Otlgnf79u1+h0BEAWF84q+XCd56iZOI6p/xiZ+IiFZi4iciCphAJP56mOBNJBJ+h0BEARGIxF8PE7xDQ0N+h0BEARGIxF8PE6czMzN+h0BEAeHrevxeam1tLZlcowsL6JyYQPPcHOaamzHZ1YVkY6Nn8REReSEwiX/btm0YGBjI/6IqNp84gZ7hYWRCIUTSaaQjEew4fhzDPT0Y3bIFcHktnfXr17t6fCKinEAM9QDFh3s2nziB7pERNKTTaEwmEc5k0JhMoiGdRvfICDafOOF6fKdOnXL9HEREQIASP5B/OCW6sICe4WFEU6m874mmUlajsLDgamwc4ycirwQq8eer7umcmEAmVPzPoCLompx0KywiIk8FKvHHYjGEw+EV25rn5hBJp4u+L5JOo3l21s3QiIg8E6jEDwA7d+5c8ftcczPSkeJz3OlIBHMtLW6GRUTkmcAl/lgsho6OjqXfJ7u6EMpkir5HVDHR2el2aLx7l4g8EbjEDwB9fX1LPycbGzHc04NkQ0PefZMNDRjp7kbKg3r+48ePu34OIiLPE7+IvFdEnhWR10TkVRH5nNcxACsrfEa3bLGSeySChWgUi6EQFqJRpCIRjHR3W3X8Hkgmk+z1E5Hr/LiBKw3gD1T170WkBcC4iDytqq95GcSKG7pEMLp1K1687jp0Tk6ieXYWcy0tmOjs9KSnv9yTTz5ZF0tMEFH98jzxq+oZAGeyP8+KyASA9wDwNPHHYjG89NJLmJqaWtqWbGzEy9dc42UYF5mfn/f1/ERkPl/H+EVkHYBrATyf57XbRWRMRMamp6ddOf/ysf5a0t/f73cIRGQw3xK/iDQD+DaAz6vqP69+XVUPqWpcVePt7e2uxbF79+6K3xtdWMDGkyfx4RMnsPHkSUQdurt3amoKg4ODjhyLiGg1XxZpE5EGWEn/G6paYOU0b8RiMZw+fRpjY2P23+TBom5jY2O48sorOd5PRI7zo6pHAPwvABOq+qdenz+f3t7eFbX9pXi1qNvAwACrfIjIcX4M9WwGcBuAG0XkZPbrFh/iWKGvrw/xeLzkfl4v6jYwMMBhHyJylOeJX1VPqKqo6kZV/bfZrye8jiOf3t5e7N+/H21tbQX38WNRt7GxMSZ/InJMIO/cLWXPnj0Fk79fi7qNjY3hwIEDHPohoqoF5glc5dqzZw8efvhhnDt3bsX23KJu4WSy4HvtLOpWyWMeVRUDAwMYGBhAU1MTtm/fzslfIiqbqKrfMZQUj8e1rKobBw0ODq6o+IkuLOCuhx5CQ5Fef7KhAV+56678d/0WqAgKZTIVVQR1dHTU7P0IROQvERlX1YsmL5n4berv71+6y3fz976H7pGRvBO8uUXdRrduzXucat5bTDQaxY4dO/gJgIiWFEr8HOqxqa+vD4lEAgMDA0uLtuXrtRdb1C1XEVTo00KuIuiF664ra42g6MICOk+exNlnnsGPs8NGaGlhQ0BEeTHxlyEWiyEWi6G/vx+jImUv6lZORZCtNYPyDRuFw/jIsWOYWrcOrzz/PI6///1L526em8O/tLRgza234ubf+q2yrp2IzMHEX4HcmHoikcCxY8eQKlDTv5rTFUHLbyTLCWcfKvNvfvITdExN4aOPPw6IIB2JvPPJ5NgxfOcv/mLFfEJVQ0Wzs8Dhw8DZs8DatcCuXQCfWEZUs5j4q5D7BADYawScqggCSg8bCYBI7sliqkvnzH3vHhkBgKX5hGQyuVQxVMyKaiJV4OBB4IEHgEwGSKWAhgbgjjuAffuAvXurXrrCVqPChoeoLJzcdcnqaiDAgYqgZTaePIlbnngCjUUakVLsnquQzd/7Hm549tmlTxnLLYZCePaGGzC6dSuamprwgQ98AKdOncLMzAxaW1uxbdu24p8uljcq4TBw4QKwZg2wuPhOowKU3md5w1MLDYTdGNyOtRb+FuQ6VvX4ZHk1EOBcVc+HT5zAjc88kzfp2rUQjeKJ3t6KnkEQXVjA3V/+ctHzL4ZC+PIXv1hRw2Ln7wSg5D4vbtuGHb29iA0O2m8g3GCnIROxv5/bcdSCYo1TEBouB66RVT0+WV1jP3j8OEZQfkXQanaGjUqp5g7jDYlEyYfUhzIZbEgk8JKNNZCWs1X9NDwMAWxVSP3iC1/A+1Y3EHNzAIDk/v04f/Yszv3e7xUdqovH4+jt7V36Pd8nukL7AgAOHkTmwAGELlwoGMPlX/2qlZQffBBY/kCeZfuNDA2t6BjkPVcRI729+NB3vpP3b4EHH7S+33OP7eO5olDjlBtCVLVizfdaLTVclSp1/Q5cIxO/x3p37AB27LBa8yNHgDNnEL78cmDXLvx8YABY9umgmMmuLuyo8uHsducT8umwGefVU1NlJ3471U9i45OqiiCWSJRsRC575BH85aWXFv1kkkvyvb29RZP+6n0BALOzFyf9PDFM7NyJrgceWJn0V+23utz3onMV8Wdf+hLufPrpwkONb79tJdTPfAZobi55PAAX3d3e1taGPXv22HpvQUUav8y+fQiJAMuvwcGGa3BwEOPj41BViAg2bdpUVsPqiCLX71TjzLV6/NLSAtx2G3D33db35mb09fVh//79K74KrRiabGzEcE8Pkg0NFYcgqpjo7KzszS4OEdqpfgpnMggvLhbdJ5JOo2NqyrFF9cbHx1d8t7MvAODwYaRLfDpSEaTuvdfq4ZUZq514AOC9Y2Ml/xYIhazhBRvyLWly7tw5PPzww7ben9fsrNXTffvt/OEtLq5M+svlGq5ckixTrkHPDX+rqvcLJJa4/mqvMYc9/hrX29tbuMeR/UiYOXAAmVQK4cVF2P0AmBsDr3Rid6qjAx94rfRjkl+/+uqyj21nGGsxm8BCRZJ/OmL983aqhHZ5QrC7LwDg7FmEbcSw5tw562N9mbHanaez06DiwgXgzBlbx1ud9Ettt+Xw4ZKNX1G5huu228p+a6EGdHx83Ltev53rr+Ialw5R8TvJfyLAPfcg9OabiDz6KOTmm61yyksvBSKRd743NFgf3SMRoLkZmTVrMHr99bbnE/J5ZeNGZEqMMy6GQnhlw4ayjz3Z1VVy/kBFSo5ziipe7+hYagAKsTvkJdnziY3x1RX7rF2LRRsxXGhrs8Zyy4zVTjzAOw1qUWvWAJdfbut4rjh7tmTjV1QZDddqhRpQTwtg7Fx/FdeYwx6/CVpagL4+62vZ3AGycwdQXbEttGsXbmhuxg3Vnre5Gdi/P+9H73QohO9ef31Fnyhyw1hOVPW8Goth+1NPFT2f3SGvTZs2LX0vVWWW2xcAsGsXIp/+dMkYGr70JWv+p8xYV5yriJ/F4wiVmhfKZKx/M35Zu9ZqfCodyqii4RKRvEnebsPqCDvX70DjzMRvmtzcwWpVfCws6J57rF73gQNW45K7gUsEkfvuw0179+KmSv/TFCk7jO7bh5v27sXg4GDeCqmGUAjR7PkXnngCwy+8ULSBOH/nnfjIrbfarurJfbdd1dPSgtB99xWc4M3F0HXjjVbVxoMP5h3jzTc8V05Vz2fvvRcjo6MXV/XkXHIJ8Id/aHtit62tLe+wTrEHGZW0a5dVvVKpKhquQg263YbVEXau34HGmXX8VL18nzJsJg9Hjl1qn1qoXTe0jt+Vqp4//uOCjV8mErHGp/PNVeQarnqv6ily/eVeI2/gInKzgXI6BrdjrYW/RSGlGqd8dfy1eANapRxsnJn4iai+FGucarnhcooD18jET0QUMIUSP8s5iYgChomfiChgmPiJiAKmLsb4RWQawD/4HYdL2gBUcY97XeA11j/Trw8w8xqvUtX21RvrIvGbTETG8k2+mITXWP9Mvz4gGNeYw6EeIqKAYeInIgoYJn7/HfI7AA/wGuuf6dcHBOMaAXCMn4gocNjjJyIKGCZ+IqKAYeL3iIjcLCI/FJEfi8jeIvv9poioiNRdWZmdaxSR/ygir4nIqyLyTa9jrEap6xORK0XkWRF5SUReFpFb/IizUiLyqIi8KSKvFHhdROTPstf/soh80OsYq2XjGn87e20JEfk7EbnG6xg9oar8cvkLQBjATwBcDSAK4AcA3p9nvxYAIwCeAxD3O26nrxHAegAvAXhX9vdf8Ttuh6/vEIA7sz+/H8BP/Y67zGvsBvBBAK8UeP0WAE8CEAAfAvC83zG7cI0fXvbvc3s9XqOdL/b4vXEdgB+r6uuqmgTw1wB25tnvAQBfBlDFQ0d9Y+ca/xOAh1X1/wGAqr7pcYzVsHN9CuCXsj+3Avi5h/FVTVVHALxVZJedAPrV8hyAXxYRHx/QW75S16iqf5f79wmrA3aFJ4F5jInfG+8B8LNlv7+R3bYk+7H5vao66GVgDip5jQB+DcCvicioiDwnIjd7Fl317FzfHwH4uIi8AeAJAJ/xJjTP2PkbmORTsD7hGIfP3K0BIhIC8KcAPuFzKG6LwBruuR5WT2pERGKq+k++RuWcWwF8XVX/RER+HcD/FpENqprxOzAqj4jcACvxb/E7Fjewx++NfwTw3mW/X5HdltMCYAOA74rIT2GNnz5eZxO8pa4RsHqIj6tqSlWnAPwIVkNQD+xc36cA/F8AUNXvA1gDa+EvU9j5G9Q9EdkI4GsAdqrqeb/jcQMTvzdeBLBeRDpEJArgYwAez72oqjOq2qaq61R1HayxxY+qaj09dqzoNWYdgdXbh4i0wRr6ed3LIKtg5/pOA9gGACLSBSvxT3sapbseB9CXre75EIAZVT3jd1BOEpErAQwAuE1Vf+R3PG7hUI8HVDUtIv8FwN/Aqg55VFVfFZEDAMZUdXUCqTs2r/FvAPx7EXkNwCKA/1ovPSqb1/cHAP6niPw+rIneT2i2PKQeiMhjsBrmtuw8xX4ADQCgqn8Oa97iFgA/BvA2gE/6E2nlbFzjfQAuA/A/xHqgeVoNXLGTSzYQEQUMh3qIiAKGiZ+IKGCY+ImIAoaJn4goYJj4iYgChomfqEwi8lkRmRCRb4vI90VkQUTu8jsuIrtYx09Uvv8M4CYASQBXAfgNf8MhKg97/ERlEJE/h7U085MAfltVXwSQ8jcqovKwx09UBlW9I7uq6A2qes7veIgqwR4/EVHAMPETEQUMEz8RUcBwkTaiMmWfmRCHNUc2ButxixkAc7Cew/vP/kVHVBoTPxFRwHCoh4goYJj4iYgChomfiChgmPiJiAKGiZ+IKGCY+ImIAoaJn4goYP4/sbgrl3Aqet0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}